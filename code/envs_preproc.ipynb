{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we prepare 2 types of preprocessing for the envrironments dataset.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to add paths of `conabio_ml` and `conabio_ml_text` to your PYTHONPATH with\n",
    "`export PYTHONPATH=`pwd`:`pwd`/conabio_ml_text/conabio_ml:`pwd`/conabio_ml_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you must have the paths of both conabio_ml and conabio_ml_text libs\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydash\n",
    "import json\n",
    "\n",
    "import conabio_ml\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from conabio_ml_text.datasets.dataset import Dataset, Partitions\n",
    "from conabio_ml_text.preprocessing.preprocessing import Tokens, PreProcessing\n",
    "from conabio_ml_text.preprocessing.transform import Transform\n",
    "\n",
    "from conabio_ml.utils.logger import get_logger, debugger\n",
    "from conabio_ml_text.utils.constraints import TransformRepresentations, LearningRates, Optimizers\n",
    "\n",
    "from model import simple_preprocess\n",
    "from preprocessing import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = get_logger(__name__)\n",
    "debug = debugger.debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the model performance using the BPE preproc, we will create 2 datasets based on the `prunned_dataset_X.csv` file produced in `eda.ipynb`.\n",
    "\n",
    "Both of them will be partitioned and the use in the `envs.ipynb/pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "base_dataset_path = Path(f\"dataset\")\n",
    "base_config_path = Path(\"configs\")\n",
    "\n",
    "dataset_path = Path(base_dataset_path) / 'dataset_multilabel.csv'\n",
    "results_path = Path(f\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we only create a dataset using a simple preprocessing that only makes the following:\n",
    "- lowercase\n",
    "- number remotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv(dataset_path)\n",
    "dataset = PreProcessing.preprocess(dataset,\n",
    "                                build_vocab=False,\n",
    "                                preprocess_fn=simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.unique(dataset.data[\"label\"])\n",
    "pprint(labels)\n",
    "NUM_LABELS = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = dataset.data[\"item\"]\n",
    "tokens = pydash.chain(items)\\\n",
    "    .map(lambda x: set(x.split()))\\\n",
    "    .reduce(lambda x, y: x.union(y), set())\\\n",
    "    .value()\n",
    "total_tokens = len(tokens)\n",
    "pprint(f\"It broadly contains {total_tokens} tokens. They will be considered to build the vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the basic config\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Sentence length. Getting by \\approx mean(word_count) + std(word_count)\n",
    "# Getting from `eda.ipynb`\n",
    "SPAN_SENTENCES = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.data), len(dataset.data[\"item\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = base_config_path / \"simple_proc_multilabel\"\n",
    "destination_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_processed_dataset = Dataset.from_csv(dataset_path)\n",
    "non_processed_dataset = PreProcessing.preprocess(non_processed_dataset,\n",
    "                                                 build_vocab=True,\n",
    "                                                 preprocess_fn=simple_preprocess,\n",
    "                                                 vocab_args = {\n",
    "                                                     \"word_size\": VOCAB_SIZE,\n",
    "                                                     \"field\": \"item\"\n",
    "                                                 })\n",
    "non_processed_dataset = Dataset.split(non_processed_dataset,\n",
    "                        train_perc=0.8,\n",
    "                        test_perc=0.1,\n",
    "                        val_perc=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`simple_proc_multilabel/dataset.csv` only contains tokens with simple processing and is constrained to the top `20K` most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = f'{destination_path}/dataset.csv'\n",
    "non_processed_dataset.to_csv(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also persist the vocabulary obtained in the preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = non_processed_dataset.representations[\"vocab\"]\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "vocab_filepath = f\"{destination_path}/vocab\"\n",
    "with open(vocab_filepath, mode=\"w\") as _f:\n",
    "    _f.write(\"\\n\".join(vocab))\n",
    "    \n",
    "pprint(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the basic config template to train the model.\n",
    "\n",
    "Note: Some params will be changed in the actual training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filepath = f\"{destination_path}/config.json\"\n",
    "CONFIG_SETTINGS = {\n",
    "    \"vocab\": vocab_filepath,\n",
    "    \"dataset\": dataset_filepath,\n",
    "    \"layers\": {\n",
    "        \"input\": {\n",
    "            \"T\": SPAN_SENTENCES\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"V\": VOCAB_SIZE,\n",
    "            \"D\": 200\n",
    "        },\n",
    "        \"lstm1\": {\n",
    "            \"M\": 16,\n",
    "            \"dropout\": 0.5\n",
    "        },\n",
    "        \"lstm2\": None,\n",
    "        \"dense_1\": {\n",
    "            \"M\": 64,\n",
    "            \"dropout\": 0.5\n",
    "        },\n",
    "        \"dense_2\":{\n",
    "            \"K\": NUM_LABELS\n",
    "        }\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"initial_learning_rate\": 1e-4,\n",
    "        \"decay_steps\": 200,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 7,\n",
    "        \"hamming_loss_threshold\": 0.7,\n",
    "        \"multilabel_threshold\": 0.7,\n",
    "        \"multilabel_classes\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(config_filepath, mode=\"w\") as _f:\n",
    "    json.dump(dict(CONFIG_SETTINGS), _f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(CONFIG_SETTINGS[\"dataset\"])\n",
    "ds.data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also preprocess the original dataset using the BPE algorithm.\n",
    "\n",
    "Then, in training we will compare the performance of both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just adding and extra param: `num_merges`\n",
    "NUM_MERGES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = base_config_path / \"bpe_multilabel\"\n",
    "destination_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = BPE.preprocess(non_processed_dataset,\n",
    "                                   preprocess_args={\"field\": \"item\",\n",
    "                                                    \"num_merges\": NUM_MERGES},\n",
    "                                   vocab_args = {\n",
    "                                                     \"word_size\": VOCAB_SIZE,\n",
    "                                                     \"field\": \"item\"\n",
    "                                                 })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that we have more tokens in the vocab. That's because we're creating new tokens in the merge process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =processed_dataset.representations[\"vocab\"]\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "V = VOCAB_SIZE\n",
    "\n",
    "pprint(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = f'{destination_path}/dataset.csv'\n",
    "processed_dataset.to_csv(destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, persisting auxiliar files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_filepath = f\"{destination_path}/vocab\"\n",
    "with open(vocab_filepath, mode=\"w\") as _f:\n",
    "    _f.write(\"\\n\".join(vocab))\n",
    "    \n",
    "pprint(VOCAB_SIZE)\n",
    "\n",
    "config_filepath = f\"{destination_path}/config.json\"\n",
    "CONFIG_SETTINGS = {\n",
    "    \"vocab\": vocab_filepath,\n",
    "    \"dataset\": dataset_filepath,\n",
    "    \"preprocessing\": {\n",
    "        \"num_merges\": NUM_MERGES\n",
    "    },\n",
    "    \"layers\": {\n",
    "        \"input\": {\n",
    "            \"T\": SPAN_SENTENCES\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"V\": VOCAB_SIZE,\n",
    "            \"D\": 200\n",
    "        },\n",
    "        \"lstm1\": {\n",
    "            \"M\": 8\n",
    "        },\n",
    "        \"lstm2\": None,\n",
    "        \"dense\": {\n",
    "            \"K\": NUM_LABELS\n",
    "        }\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"initial_learning_rate\": 0.02,\n",
    "        \"decay_steps\": 200,\n",
    "        \"clipvalue\": 0.3,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 7,\n",
    "        \"multilabel_threshold\": 0.3,\n",
    "        \"multilabel_classes\": 3\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(config_filepath, mode=\"w\") as _f:\n",
    "    json.dump(dict(CONFIG_SETTINGS), _f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(CONFIG_SETTINGS[\"dataset\"])\n",
    "ds.data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
