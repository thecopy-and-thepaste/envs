{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we perform the training process for the environment classifier, defined in the `model.py` script.\n",
    "\n",
    "We will use two dataset for this purpose:\n",
    "\n",
    "- `config/bpe/dataset.csv`. This dataset was preprocessed with the BPE algorithm, and its vocab is defined in config/bpe/vocab.json\n",
    "- `config/simple_proc/dataset.csv`. This dataset preprocessing only consists in lowercasing and number remotion. Its vocab is defined in config/simple_proc/vocab.json\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from pprint import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you must have the paths of both conabio_ml and conabio_ml_text libs\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to update the PYTHON_PATH to\n",
    "# export PYTHONPATH=`pwd`:`pwd`/conabio_ml_text/conabio_ml:`pwd`/conabio_ml_text\n",
    "\n",
    "from conabio_ml_text.datasets.dataset import Dataset, Partitions\n",
    "from conabio_ml_text.preprocessing.preprocessing import Tokens\n",
    "from conabio_ml_text.preprocessing.transform import Transform\n",
    "\n",
    "from conabio_ml_text.trainers.bcknds.tfkeras import TFKerasTrainer, TFKerasTrainerConfig\n",
    "from conabio_ml_text.trainers.bcknds.tfkeras import CHECKPOINT_CALLBACK, TENSORBOARD_CALLBACK\n",
    "\n",
    "from conabio_ml.evaluator.generic.evaluator import Evaluator, Metrics\n",
    "\n",
    "from conabio_ml_text.utils.constraints import TransformRepresentations as TR, LearningRates as LR\n",
    "from conabio_ml_text.trainers.builders import create_learning_rate\n",
    "\n",
    "from model import EnvironmentClassifier,  multilabel_topk, HammingLoss, multilabel_converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by using a config file that contains all the params/hyperparams to load the dataset and to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = Path(\"configs/simple_proc_multilabel/config.json\")\n",
    "config = {}\n",
    "with open(config_file) as _f:\n",
    "    config = json.load(_f)\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(f\"results\")\n",
    "dataset_filepath = Path(f\"{config['dataset']}\")\n",
    "vocab_filepath = Path(f\"{config['vocab']}\")\n",
    "\n",
    "# Model layers\n",
    "layers = config[\"layers\"]\n",
    "\n",
    "#Params\n",
    "initial_lr = config[\"params\"][\"initial_learning_rate\"]     # Learning rate\n",
    "decay_steps = config[\"params\"][\"decay_steps\"]              # Decay steps for the lr\n",
    "batch_size = config[\"params\"][\"batch_size\"]                # Batch size\n",
    "epochs = config[\"params\"][\"epochs\"]                        # Epochs of training\n",
    "\n",
    "hamming_loss_th = config[\"params\"][\"hamming_loss_threshold\"]\n",
    "\n",
    "multilabel_th = config[\"params\"][\"multilabel_threshold\"]\n",
    "multilabel_k_classes = config[\"params\"][\"multilabel_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and the vocab file that coresponds to each preprocessing method.\n",
    "\n",
    "- Simple preprocessing: `configs/simple_proc`\n",
    "- BPE preprocessing: `configs/bpe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv(dataset_filepath)\n",
    "dataset = Transform.as_data_generator(dataset,\n",
    "                                      vocab=vocab_filepath,\n",
    "                                      shuffle=True,\n",
    "                                      transform_args={\n",
    "                                          \"pad_length\": layers[\"input\"][\"T\"],\n",
    "                                          \"unk_token\": Tokens.UNK_TOKEN\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just for testing purposes\n",
    "#vocab = dataset.representations[\"vocab\"]\n",
    "#test_datagen = dataset.representations[\"data_generators\"][\"test\"]\n",
    "#test = dataset.get_partition(\"test\")[\"item\"]\n",
    "#pprint(\"TEST\")\n",
    "#pprint(test.iloc[0])\n",
    "#pprint(\"-------\")\n",
    "#pprint(\"FROM DATAGEN\")\n",
    "#print([vocab[x] for x in next(test_datagen())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REPRESENTATION = TR.DATA_GENERATORS\n",
    "lr_schedule = create_learning_rate({\"initial_learning_rate\": initial_lr,\n",
    "                                    \"decay_steps\": decay_steps },\n",
    "                                   learning_rate_name=LR.EXPONENTIAL_LR)\n",
    "\n",
    "trainer_config = TFKerasTrainerConfig.create(config={\n",
    "    \"strategy\": None,\n",
    "    \"callbacks\": {\n",
    "        CHECKPOINT_CALLBACK: {\n",
    "            \"filepath\": os.path.join(results_path, \"checkpoints\"),\n",
    "            \"save_best_only\": False\n",
    "        },\n",
    "        TENSORBOARD_CALLBACK: {\n",
    "            \"log_dir\": os.path.join(results_path, \"tb_logs\")\n",
    "        }}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we had created a set of properties in the Dataset\n",
    "# - representations[TRAIN_REPRESENTATION]: Datagenerator\n",
    "# - representations[\"vocab\"]: Dictionary to convert to tensors the dataset,\n",
    "#   calculated using the custom preprocessing BPE\n",
    "# debug(\"dataset\", dataset)\n",
    "\n",
    "print(f'Vocab size {layers[\"embedding\"][\"V\"]}')\n",
    "print(f\"From dataset {len(dataset.representations['vocab'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnvironmentClassifier.create(model_config={\n",
    "    \"ENV_CLASSIFIER\": {\n",
    "        \"layers\": {\n",
    "            \"input\": layers[\"input\"],\n",
    "            \"embedding\": layers[\"embedding\"],\n",
    "            \"lstm_1\": layers[\"lstm1\"],\n",
    "            \"lstm_2\": layers[\"lstm2\"],\n",
    "            \"dense_1\": layers[\"dense_1\"],\n",
    "            \"dense_2\": layers[\"dense_2\"]\n",
    "        }\n",
    "    }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = TFKerasTrainer.train(dataset=dataset,\n",
    "                                     model=model,\n",
    "                                     execution_config=trainer_config,\n",
    "                                     train_config={\n",
    "                                         \"ENV_CLASSIFIER\": {\n",
    "                                             \"representation\": TRAIN_REPRESENTATION,\n",
    "                                             'optimizer': tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                                             'loss': tf.keras.losses.BinaryCrossentropy(),\n",
    "                                             \"batch_size\": batch_size,\n",
    "                                             \"epochs\": epochs,\n",
    "                                             \"metrics\": [multilabel_topk(multilabel_k_classes), 'accuracy']\n",
    "                                         }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = trained_model.predict(dataset=dataset,\n",
    "                                        execution_config=None,\n",
    "                                        prediction_config={\n",
    "                                            \"pred_converter_fn\": multilabel_converter(multilabel_th)\n",
    "                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Evaluator.eval(dataset,\n",
    "                         predict_dataset,\n",
    "                         {\"metrics_set\": {\n",
    "                             Metrics.Sets.MULTILABEL: {\n",
    "                                 'per_class': True,\n",
    "                                 'average': 'micro',\n",
    "                                 \"zero_division\": 1.0\n",
    "                             }\n",
    "                         },\n",
    "                          \"dataset_partition\": Partitions.TEST\n",
    "                         })\n",
    "pprint(metrics.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
