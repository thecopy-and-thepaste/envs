{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is addressed to explore some insights about the dataset that contains abstracts gathered from PLOS.\n",
    "\n",
    "And then tune up to use it in a classifier whose main purpose is to detect the following environments: `['coastal', 'freshwater', 'marine', 'terrestrial'] `\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to add paths of `conabio_ml` and `conabio_ml_text` to your PYTHONPATH with\n",
    "`export PYTHONPATH=`pwd`:`pwd`/conabio_ml_text/conabio_ml:`pwd`/conabio_ml_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you must have the paths of both conabio_ml and conabio_ml_text libs\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydash\n",
    "import json\n",
    "\n",
    "import conabio_ml\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "from conabio_ml.pipeline import Pipeline\n",
    "from conabio_ml.assets import AssetTypes\n",
    "from conabio_ml_text.datasets.dataset import Dataset, Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset_path = Path(f\"dataset\")\n",
    "dataset_path = Path(f\"{base_dataset_path}/plos_2021-01-06.csv\")\n",
    "results_path = Path(f\"results\")\n",
    "report_path = Path(f\"report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset to know the following:\n",
    "- Number of classes\n",
    "- Stats about the item column (abtract): Min/max/mean number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the following labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.unique(dataset.data[\"label\"]).tolist()\n",
    "pprint(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the clases in 2 types: \n",
    "- domain samples: ['coastal', 'freshwater', 'marine', 'terrestrial']\n",
    "- out-of-domain-samples: ['health-sciences', 'earth-sciences', 'life-sciences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = dataset.data[\"item\"].apply(lambda x: x.split())\n",
    "word_count = items.apply(lambda x: len(x))\n",
    "\n",
    "pprint (f\"Max: {np.max(word_count)}, Min:{np.min(word_count)}, Mean:{np.mean(word_count)}, Std:{np.std(word_count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some examples of the items with maximum and minimum `num_of_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max sentence:\\n {dataset.data.loc[int(np.argmax(word_count))]['item']}\")\n",
    "print(\"------------\")\n",
    "print(f\"Min sentence:\\n {dataset.data.loc[int(np.argmin(word_count))]['item']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we constrain the min number of words of the samples.\n",
    "\n",
    "That's because, after the preproc our samples will be transformed to tensors (`[w0, w1, w2] -> [ix, ix, ix]`) of fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NUM_WORDS = 16\n",
    "min_dataset_path = base_dataset_path / \"min_words_dataset.csv\"\n",
    "\n",
    "ix_min_words = dataset.data[\"item\"].apply(lambda x: len(x.split()) >= MIN_NUM_WORDS)\n",
    "min_words_dataset = dataset.data[ix_min_words].reset_index()\n",
    "min_words_dataset.to_csv(min_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the min words dataset. We draw the same stats as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv(min_dataset_path)\n",
    "dataset.reporter(report_path / \"dataset\", {})\n",
    "\n",
    "items = dataset.data[\"item\"].apply(lambda x: x.split())\n",
    "word_count = items.apply(lambda x: len(x))\n",
    "\n",
    "pprint (f\"Max: {np.max(word_count)}, Min:{np.min(word_count)}, Mean:{np.mean(word_count)}, Std:{np.std(word_count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We broadly have the following statsfor the quantity of words:\n",
    "\n",
    "- Max: 1524\n",
    "- Min: 16\n",
    "- Mean: ~261\n",
    "- Std: ~89\n",
    "\n",
    "We need to calculate the amount of samples over some values of `word_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH_1 = 261 \n",
    "TH_2 = 261 + (1 * 89)\n",
    "TH_25 = 261 + int(1.5 * 89)\n",
    "TH_3 = 261 + (2 * 89)\n",
    "(f\"With {TH_1} words: {len(word_count[word_count.apply(lambda x: x < TH_1)]) / len(word_count)} of the dataset\",\n",
    " f\"{TH_2} words: {len(word_count[word_count.apply(lambda x: x < TH_2)]) / len(word_count)} of the dataset\",\n",
    " f\"{TH_25} words: {len(word_count[word_count.apply(lambda x: x < TH_25)]) / len(word_count)} of the dataset\",\n",
    " f\"{TH_3} words: {len(word_count[word_count.apply(lambda x: x < TH_3)]) / len(word_count)} of the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for padding purposes in the train we get a sample size of `450` words.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we drop one class of the out-of-domain dataset (`[health-sciences, life-sciences, earth-sciences]`) to balance the dataset. \n",
    "\n",
    "Having the following labels to classify.\n",
    "\n",
    "`['coastal', 'freshwater', 'marine', 'terrestrial', 'other'] `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove 1 (earth-sciences) of the ood classes\n",
    "classes = ['health-sciences', \"life-sciences\", \"terrestrial\", \"marine\", \"freshwater\", \"coastal\"]\n",
    "result_labels = [\"other\", \"terrestrial\", \"marine\", \"freshwater\", \"coastal\"]\n",
    "dataset_5classes = dataset.data[dataset.data[\"label\"].isin(classes)]\n",
    "\n",
    "labels_to_group = ['health-sciences', \"life-sciences\"]\n",
    "\n",
    "# Some items might be repeated in the sets\n",
    "hs_rows = dataset_5classes[dataset_5classes[\"label\"] == 'health-sciences']\n",
    "temp = dataset_5classes[dataset_5classes[\"label\"] == 'life-sciences']\n",
    "ls_rows = temp[~temp[\"DOC_ID\"].isin(hs_rows[\"DOC_ID\"])]\n",
    "\n",
    "ixs = set(hs_rows.index).union(set(ls_rows.index))\n",
    "dataset_5classes.loc[ixs, \"label\"] = \"other\"\n",
    "\n",
    "prunned_dataset = dataset_5classes[dataset_5classes[\"label\"].isin(result_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.unique(prunned_dataset[\"label\"])\n",
    "unique_items = pd.unique(prunned_dataset[\"DOC_ID\"])\n",
    "\n",
    "len(unique_items), len(prunned_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains fewer unique items, so, some of the labels are repeated across the dataset.\n",
    "\n",
    "We produce 2 datasets with this info.\n",
    "\n",
    "- The simplified version of the dataset with one class for each item.\n",
    "- The multilabel version of the dataset, using all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = OrderedDict()\n",
    "simplified_dataset = pd.DataFrame()\n",
    "doc_ids = set()\n",
    "\n",
    "for label in labels:\n",
    "    amount = len(prunned_dataset[prunned_dataset[\"label\"] == label])\n",
    "    samples_count[amount] = label\n",
    "    \n",
    "\n",
    "for sample_count, label in samples_count.items():\n",
    "    # Items with the current label\n",
    "    temp_dataset = prunned_dataset[prunned_dataset[\"label\"] == label]\n",
    "    \n",
    "    # Unique doc_ids\n",
    "    unique_items = set(temp_dataset[\"DOC_ID\"])\n",
    "    label_items = unique_items - doc_ids\n",
    "    \n",
    "    temp_dataset = temp_dataset[temp_dataset[\"DOC_ID\"].isin(label_items)]\n",
    "    simplified_dataset = simplified_dataset.append(temp_dataset)\n",
    "    \n",
    "    doc_ids = doc_ids.union(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_dataset.to_csv(base_dataset_path / \"dataset_multiclass.csv\")\n",
    "prunned_dataset.to_csv(base_dataset_path / \"dataset_multilabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Dataset.from_csv(base_dataset_path / \"dataset_multiclass.csv\")\n",
    "res.reporter(report_path / \"dataset\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Dataset.from_csv(base_dataset_path / \"dataset_multilabel.csv\")\n",
    "res.reporter(report_path / \"dataset_merged\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
